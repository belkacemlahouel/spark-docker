version: '3'

networks:
  hadoop-net:

services:
  spark-master:
    build: .
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_HISTORY_OPTS=-Dspark.eventLog.enabled=true -Dspark.eventLog.dir=file:/tmp/spark-events -Dspark.history.fs.logDirectory=file:/tmp/spark-events
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    networks:
      - hadoop-net
    volumes:
      - ./src/:/app   # app code
      - ./data:/data # dataset
      - ./tmp/:/tmp  # spark shuffle
      - ./spark-events:/tmp/spark-events
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    working_dir: /app
    deploy:
      resources:
        limits:
          memory: 1g
          cpus:  '2'
    command: bash -c "spark-class org.apache.spark.deploy.master.Master -h spark-master"

  spark-worker:
    networks:
      - hadoop-net
    build: .
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_HISTORY_OPTS=-Dspark.eventLog.enabled=true -Dspark.eventLog.dir=file:/tmp/spark-events -Dspark.history.fs.logDirectory=file:/tmp/spark-events
    volumes:
      - ./src/:/app   # app code
      - ./data:/data # dataset
      - ./tmp/:/tmp  # spark shuffle
      - ./spark-events:/tmp/spark-events
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    working_dir: /app
    deploy:
      resources:
        limits:
          memory: 1g
          cpus:  '2'
    command: bash -c "spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"

  spark-history-server:
    build: .
    container_name: spark-history
    ports:
      - "18080:18080"
    networks:
      - hadoop-net
    volumes:
      - ./spark-events:/tmp/spark-events  # Persist event logs here
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    environment:
      - SPARK_MODE=history-server
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:/tmp/spark-events -Dspark.eventLog.enabled=true -Dspark.eventLog.dir=file:/tmp/spark-events
    command: bash -c "spark-class org.apache.spark.deploy.history.HistoryServer"

  hdfs-namenode:
    build:
      context: .
      dockerfile: Dockerfile.hdfs
    ports:
      - "9870:9870"  # NameNode Web UI
      - "9864:9864"  # DataNode Web UI
      - "2222:22"
      - "9000:9000"
    networks:
      - hadoop-net
    volumes:
      - ./data/namenode:/hadoop/dfs/name
      - ./data/datanode:/hadoop/dfs/data
    environment:
      - JAVA_HOME=/usr/local/openjdk-11
    restart: unless-stopped
    container_name: hdfs-single-node

  airflow:
    depends_on:
      - airflow-db
    image: apache/airflow:2.10.2-python3.10
    container_name: airflow-all
    restart: always
    ports:
      - "8081:8080"
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        exec airflow webserver &
        exec airflow triggerer &
        exec airflow scheduler
      "

  airflow-db:
    container_name: airflow-db
    image: postgres:14
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:

